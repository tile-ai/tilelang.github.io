<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="General Matrix-Matrix Multiplication with Dequantization" href="matmul_dequant.html" /><link rel="prev" title="General Matrix-Vector Multiplication (GEMV)" href="gemv.html" />

    <!-- Generated with Sphinx 5.2.3 and Furo 2023.03.27 -->
        <title>General Matrix-Matrix Multiplication with Tile Library - Tile Language 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?digest=fad236701ea90a88636c2a8c73b44ae642ed2a53" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">Tile Language <br> 0.1.0 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo only-light" src="../_static/img/logo-row.svg" alt="Light Logo"/>
    <img class="sidebar-logo only-dark" src="../_static/img/logo-row.svg" alt="Dark Logo"/>
  </div>
  
  <span class="sidebar-brand-text">Tile Language <br> 0.1.0 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">GET STARTED</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/Installation.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/overview.html">The Tile Language: A Brief Introduction</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">TUTORIALS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/writing_kernels_with_tilelibrary.html">Writing High-Performance Kernels with the Tile Library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/writing_kernels_with_thread_primitives.html">Writing High-Performance Kernels with Thread Primitives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/annotate_memory_layout.html">Annotate Memory Layout</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/debug_tools_for_tilelang.html">Debugging Tile Language Programs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/auto_tuning.html">Auto-Tuning Techniques for Performance Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/jit_compilation.html">Just In Time Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/pipelining_computations_and_data_movements.html">Pipelining Computation and Data Movement</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DEEP LEARNING OPERATORS</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="elementwise.html">ElementWise Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="gemv.html">General Matrix-Vector Multiplication (GEMV)</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">General Matrix-Matrix Multiplication with Tile Library</a></li>
<li class="toctree-l1"><a class="reference internal" href="matmul_dequant.html">General Matrix-Matrix Multiplication with Dequantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="flash_attention.html">Flash Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="flash_linear_attention.html">Flash Linear Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="convolution.html">Convolution</a></li>
<li class="toctree-l1"><a class="reference internal" href="tmac_gpu.html">TMAC: Look Up Table Based Mixed Precision Computing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">LANGUAGE REFERENCE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../language_ref/ast.html">Tile Language AST</a></li>
<li class="toctree-l1"><a class="reference internal" href="../language_ref/primitives.html">Tile Language: Primitives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../language_ref/tilelibrary.html">Tile Language: TileLibrary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../privacy.html">Privacy</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="general-matrix-matrix-multiplication-with-tile-library">
<h1>General Matrix-Matrix Multiplication with Tile Library<a class="headerlink" href="#general-matrix-matrix-multiplication-with-tile-library" title="Permalink to this heading">#</a></h1>
<div style="text-align: left;">
    <em>Author:</em> <a href="https://github.com/LeiWang1999">Lei Wang</a>
</div>
<div class="myclass1 myclass2 admonition warning" id="a-tip-reference">
<p class="admonition-title">Warning</p>
<p>This document is still <strong>experimental</strong> and may be incomplete.<br />
Suggestions and improvements are highly encouraged—please submit a PR!</p>
</div>
<p>TileLang is a domain-specific language (DSL) designed for writing high-performance GPU kernels. It provides three main levels of abstraction:</p>
<ul class="simple">
<li><p><strong>Level 1:</strong> A user writes pure compute logic without knowledge of or concern for hardware details (e.g., GPU caches, tiling, etc.). The compiler or runtime performs automatic scheduling and optimization. This level is conceptually similar to the idea behind TVM.</p></li>
<li><p><strong>Level 2:</strong> A user is aware of GPU architecture concepts—such as shared memory, tiling, and thread blocks—but does not necessarily want to drop down to the lowest level of explicit thread control. This mode is somewhat comparable to Triton’s programming model, where you can write tile-level operations and let the compiler do layout inference, pipelining, etc.</p></li>
<li><p><strong>Level 3:</strong> A user takes full control of thread-level primitives and can write code that is almost as explicit as a hand-written CUDA/HIP kernel. This is useful for performance experts who need to manage every detail, such as PTX inline assembly, explicit thread behavior, etc.</p></li>
</ul>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="../_images/overview.png"><img alt="Overview" src="../_images/overview.png" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-text">Figure 1: High-level overview of the TileLang compilation flow.</span><a class="headerlink" href="#id1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>In this tutorial, we introduce Level 2 with a matrix multiplication example in TileLang. We will walk through how to allocate shared memory, set up thread blocks, perform parallel copying, pipeline the computation, and invoke the tile-level GEMM intrinsic. We will then show how to compile and run the kernel in Python, comparing results and measuring performance.</p>
<section id="why-another-gpu-dsl">
<h2>Why Another GPU DSL?<a class="headerlink" href="#why-another-gpu-dsl" title="Permalink to this heading">#</a></h2>
<p>TileLang emerged from the need for a DSL that:</p>
<ol class="arabic simple">
<li><p>Balances high-level expressiveness (like TVM or Triton) with enough flexibility to control finer details when needed.</p></li>
<li><p>Supports efficient code generation and scheduling for diverse hardware backends (NVIDIA GPUs, AMD GPUs, CPU, etc.).</p></li>
<li><p>Simplifies scheduling and memory pipelines with built-in primitives (such as <code class="docutils literal notranslate"><span class="pre">T.Pipelined</span></code>, <code class="docutils literal notranslate"><span class="pre">T.Parallel</span></code>, <code class="docutils literal notranslate"><span class="pre">T.gemm</span></code>), yet retains options for expert-level tuning.</p></li>
</ol>
<p>While Level 1 in TileLang can be very comfortable for general users—since it requires no scheduling or hardware-specific knowledge—it can incur longer auto-tuning times and may not handle some complex kernel fusion patterns (e.g., Flash Attention) as easily. Level 3 gives you full control but demands more effort, similar to writing raw CUDA/HIP kernels. Level 2 thus strikes a balance for users who want to write portable and reasonably concise code while expressing important architectural hints.</p>
</section>
<section id="matrix-multiplication-example">
<h2>Matrix Multiplication Example<a class="headerlink" href="#matrix-multiplication-example" title="Permalink to this heading">#</a></h2>
<figure class="align-center">
<img alt="Matmul Example" src="../_images/MatmulExample.png" />
</figure>
<section id="basic-structure">
<h3>Basic Structure<a class="headerlink" href="#basic-structure" title="Permalink to this heading">#</a></h3>
<p>Below is a simplified code snippet for a 1024 x 1024 x 1024 matrix multiplication. It uses:</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">T.Kernel(...)</span></code></strong> to initialize the thread block configuration (grid dimensions, block size, etc.).</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">T.alloc_shared(...)</span></code></strong> to allocate GPU shared memory.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">T.alloc_fragment(...)</span></code></strong> to allocate a register fragment for accumulation.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">T.Pipelined(...)</span></code></strong> to express software pipelining across the K dimension.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">T.Parallel(...)</span></code></strong> to parallelize data copy loops.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">T.gemm(...)</span></code></strong> to perform tile-level GEMM operations (which map to the appropriate backends, such as MMA instructions on NVIDIA GPUs).</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tilelang</span>
<span class="kn">import</span> <span class="nn">tilelang.language</span> <span class="k">as</span> <span class="nn">T</span>
<span class="kn">from</span> <span class="nn">tilelang.intrinsics</span> <span class="kn">import</span> <span class="n">make_mma_swizzle_layout</span>

<span class="k">def</span> <span class="nf">matmul</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">block_M</span><span class="p">,</span> <span class="n">block_N</span><span class="p">,</span> <span class="n">block_K</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float16&quot;</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">):</span>
    <span class="nd">@T</span><span class="o">.</span><span class="n">prim_func</span>
    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span>
        <span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">),</span> <span class="n">dtype</span><span class="p">),</span>
        <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">dtype</span><span class="p">),</span>
        <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">dtype</span><span class="p">),</span>
    <span class="p">):</span>
        <span class="c1"># Initialize Kernel Context</span>
        <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">Kernel</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">ceildiv</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">block_N</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">ceildiv</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">block_M</span><span class="p">),</span> <span class="n">threads</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span> <span class="k">as</span> <span class="p">(</span><span class="n">bx</span><span class="p">,</span> <span class="n">by</span><span class="p">):</span>
            <span class="n">A_shared</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_shared</span><span class="p">((</span><span class="n">block_M</span><span class="p">,</span> <span class="n">block_K</span><span class="p">),</span> <span class="n">dtype</span><span class="p">)</span>
            <span class="n">B_shared</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_shared</span><span class="p">((</span><span class="n">block_K</span><span class="p">,</span> <span class="n">block_N</span><span class="p">),</span> <span class="n">dtype</span><span class="p">)</span>
            <span class="n">C_local</span>  <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_fragment</span><span class="p">((</span><span class="n">block_M</span><span class="p">,</span> <span class="n">block_N</span><span class="p">),</span> <span class="n">accum_dtype</span><span class="p">)</span>

            <span class="c1"># Optional layout hints (commented out by default)</span>
            <span class="c1"># T.annotate_layout({</span>
            <span class="c1">#     A_shared: make_mma_swizzle_layout(A_shared),</span>
            <span class="c1">#     B_shared: make_mma_swizzle_layout(B_shared),</span>
            <span class="c1"># })</span>

            <span class="c1"># Optional: Enabling swizzle-based rasterization</span>
            <span class="c1"># T.use_swizzle(panel_size=10, enable=True)</span>

            <span class="c1"># Clear local accumulation</span>
            <span class="n">T</span><span class="o">.</span><span class="n">clear</span><span class="p">(</span><span class="n">C_local</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">ko</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">Pipelined</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">ceildiv</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">block_K</span><span class="p">),</span> <span class="n">num_stages</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
                <span class="c1"># Copy tile of A from global to shared memory</span>
                <span class="n">T</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">by</span> <span class="o">*</span> <span class="n">block_M</span><span class="p">,</span> <span class="n">ko</span> <span class="o">*</span> <span class="n">block_K</span><span class="p">],</span> <span class="n">A_shared</span><span class="p">)</span>

                <span class="c1"># Parallel copy tile of B from global to shared memory</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">Parallel</span><span class="p">(</span><span class="n">block_K</span><span class="p">,</span> <span class="n">block_N</span><span class="p">):</span>
                    <span class="n">B_shared</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">ko</span> <span class="o">*</span> <span class="n">block_K</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span> <span class="n">bx</span> <span class="o">*</span> <span class="n">block_N</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span>

                <span class="c1"># Perform a tile-level GEMM</span>
                <span class="n">T</span><span class="o">.</span><span class="n">gemm</span><span class="p">(</span><span class="n">A_shared</span><span class="p">,</span> <span class="n">B_shared</span><span class="p">,</span> <span class="n">C_local</span><span class="p">)</span>

            <span class="c1"># Copy result from local (register fragment) to global memory</span>
            <span class="n">T</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">C_local</span><span class="p">,</span> <span class="n">C</span><span class="p">[</span><span class="n">by</span> <span class="o">*</span> <span class="n">block_M</span><span class="p">,</span> <span class="n">bx</span> <span class="o">*</span> <span class="n">block_N</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">main</span>

<span class="c1"># 1. Create the TileLang function</span>
<span class="n">func</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>

<span class="c1"># 2. JIT-compile the kernel for NVIDIA GPU</span>
<span class="n">jit_kernel</span> <span class="o">=</span> <span class="n">tilelang</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">out_idx</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">target</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># 3. Prepare input tensors in PyTorch</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

<span class="c1"># 4. Invoke the JIT-compiled kernel</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">jit_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">ref_c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">@</span> <span class="n">b</span>

<span class="c1"># 5. Validate correctness</span>
<span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">ref_c</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Kernel output matches PyTorch reference.&quot;</span><span class="p">)</span>

<span class="c1"># 6. Inspect generated CUDA code (optional)</span>
<span class="n">cuda_source</span> <span class="o">=</span> <span class="n">jit_kernel</span><span class="o">.</span><span class="n">get_kernel_source</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Generated CUDA kernel:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">cuda_source</span><span class="p">)</span>

<span class="c1"># 7. Profile performance</span>
<span class="n">profiler</span> <span class="o">=</span> <span class="n">jit_kernel</span><span class="o">.</span><span class="n">get_profiler</span><span class="p">()</span>
<span class="n">latency</span> <span class="o">=</span> <span class="n">profiler</span><span class="o">.</span><span class="n">do_bench</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Latency: </span><span class="si">{</span><span class="n">latency</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="key-concepts">
<h3>Key Concepts<a class="headerlink" href="#key-concepts" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Kernel Context</strong>:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">Kernel</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">ceildiv</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">block_N</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">ceildiv</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">block_M</span><span class="p">),</span> <span class="n">threads</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span> <span class="k">as</span> <span class="p">(</span><span class="n">bx</span><span class="p">,</span> <span class="n">by</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<ul class="simple">
<li><p>This sets up the block grid dimensions based on N/block_N and M/block_M.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">threads=128</span></code> specifies that each thread block uses 128 threads. The compiler will infer how loops map to these threads.</p></li>
</ul>
<figure class="align-center">
<img alt="Parallel" src="../_images/Parallel.png" />
</figure>
<ol class="arabic simple" start="2">
<li><p><strong>Shared &amp; Fragment Memory</strong>:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A_shared</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_shared</span><span class="p">((</span><span class="n">block_M</span><span class="p">,</span> <span class="n">block_K</span><span class="p">),</span> <span class="n">dtype</span><span class="p">)</span>
<span class="n">B_shared</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_shared</span><span class="p">((</span><span class="n">block_K</span><span class="p">,</span> <span class="n">block_N</span><span class="p">),</span> <span class="n">dtype</span><span class="p">)</span>
<span class="n">C_local</span>  <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_fragment</span><span class="p">((</span><span class="n">block_M</span><span class="p">,</span> <span class="n">block_N</span><span class="p">),</span> <span class="n">accum_dtype</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">T.alloc_shared</span></code> allocates shared memory across the entire thread block.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">T.alloc_fragment</span></code> allocates register space for local accumulation. Though it is written as <code class="docutils literal notranslate"><span class="pre">(block_M,</span> <span class="pre">block_N)</span></code>, the compiler’s layout inference assigns slices of this space to each thread.</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>Software Pipelining</strong>:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">ko</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">Pipelined</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">ceildiv</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">block_K</span><span class="p">),</span> <span class="n">num_stages</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">T.Pipelined</span></code> automatically arranges asynchronous copy and compute instructions to overlap memory operations with arithmetic.</p></li>
<li><p>The argument <code class="docutils literal notranslate"><span class="pre">num_stages=3</span></code> indicates the pipeline depth.</p></li>
</ul>
<figure class="align-center">
<img alt="Software Pipeline Inference" src="../_images/software_pipeline_inference.png" />
</figure>
<ol class="arabic simple" start="4">
<li><p><strong>Parallel Copy</strong>:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">Parallel</span><span class="p">(</span><span class="n">block_K</span><span class="p">,</span> <span class="n">block_N</span><span class="p">):</span>
    <span class="n">B_shared</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">ko</span> <span class="o">*</span> <span class="n">block_K</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span> <span class="n">bx</span> <span class="o">*</span> <span class="n">block_N</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">T.Parallel</span></code> marks the loop for thread-level parallelization.</p></li>
<li><p>The compiler will map these loops to the available threads in the block.</p></li>
</ul>
<ol class="arabic simple" start="5">
<li><p><strong>Tile-Level GEMM</strong>:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">T</span><span class="o">.</span><span class="n">gemm</span><span class="p">(</span><span class="n">A_shared</span><span class="p">,</span> <span class="n">B_shared</span><span class="p">,</span> <span class="n">C_local</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>A single call that performs a tile-level matrix multiplication using the specified buffers.</p></li>
<li><p>Under the hood, for NVIDIA targets, it can use CUTLASS/Cute or WMMA instructions. On AMD GPUs, TileLang uses a separate HIP or composable kernel approach.</p></li>
</ul>
<ol class="arabic simple" start="6">
<li><p><strong>Copying Back Results</strong>:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">T</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">C_local</span><span class="p">,</span> <span class="n">C</span><span class="p">[</span><span class="n">by</span> <span class="o">*</span> <span class="n">block_M</span><span class="p">,</span> <span class="n">bx</span> <span class="o">*</span> <span class="n">block_N</span><span class="p">])</span>
</pre></div>
</div>
<ul class="simple">
<li><p>After computation, data in the local register fragment is written back to global memory.</p></li>
</ul>
</section>
</section>
<section id="comparison-with-other-dsls">
<h2>Comparison with Other DSLs<a class="headerlink" href="#comparison-with-other-dsls" title="Permalink to this heading">#</a></h2>
<p>TileLang Level 2 is conceptually similar to Triton in that the user can control tiling and parallelization, while letting the compiler handle many low-level details. However, TileLang also:</p>
<ul class="simple">
<li><p>Allows explicit memory layout annotations (e.g. <code class="docutils literal notranslate"><span class="pre">make_mma_swizzle_layout</span></code>).</p></li>
<li><p>Supports a flexible pipeline pass (<code class="docutils literal notranslate"><span class="pre">T.Pipelined</span></code>) that can be automatically inferred or manually defined.</p></li>
<li><p>Enables mixing different levels in a single program—for example, you can write some parts of your kernel in Level 3 (thread primitives) for fine-grained PTX/inline-assembly and keep the rest in Level 2.</p></li>
</ul>
</section>
<section id="performance-on-different-platforms">
<h2>Performance on Different Platforms<a class="headerlink" href="#performance-on-different-platforms" title="Permalink to this heading">#</a></h2>
<figure class="align-center">
<img alt="Performance on Different Platforms" src="../_images/op_benchmark_consistent_gemm_fp16.png" />
</figure>
<p>When appropriately tuned (e.g., by using an auto-tuner), TileLang achieves performance comparable to or better than vendor libraries and Triton on various GPUs. In internal benchmarks, for an FP16 matrix multiply (e.g., 4090, A100, H100, MI300X), TileLang has shown:</p>
<ul class="simple">
<li><p>~1.1x speedup over cuBLAS on RTX 4090</p></li>
<li><p>~0.97x on A100 (on par with cuBLAS)</p></li>
<li><p>~1.0x on H100</p></li>
<li><p>~1.04x on MI300X</p></li>
<li><p>Compared to Triton, speedups range from 1.08x to 1.25x depending on the hardware.</p></li>
</ul>
<p>These measurements will vary based on tile sizes, pipeline stages, and the hardware’s capabilities.</p>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h2>
<p>This tutorial demonstrated a Level 2 TileLang kernel for matrix multiplication. With just a few lines of code:</p>
<ol class="arabic simple">
<li><p>We allocated shared memory and register fragments.</p></li>
<li><p>We pipelined the loading and computation along the K dimension.</p></li>
<li><p>We used parallel copying to efficiently load tiles from global memory.</p></li>
<li><p>We invoked <code class="docutils literal notranslate"><span class="pre">T.gemm</span></code> to dispatch a tile-level matrix multiply.</p></li>
<li><p>We verified correctness against PyTorch and examined performance.</p></li>
</ol>
<p>By balancing high-level abstractions (like <code class="docutils literal notranslate"><span class="pre">T.copy</span></code>, <code class="docutils literal notranslate"><span class="pre">T.Pipelined</span></code>, <code class="docutils literal notranslate"><span class="pre">T.gemm</span></code>) with the ability to annotate layouts or drop to thread primitives (Level 3) when needed, TileLang can be both user-friendly and highly tunable. We encourage you to experiment with tile sizes, pipeline depths, or explicit scheduling to see how performance scales across different GPUs.</p>
<p>For more advanced usage—including partial lowering, explicitly controlling thread primitives, or using inline assembly—you can explore Level 3. Meanwhile, for purely functional expressions and high-level scheduling auto-tuning, consider Level 1.</p>
</section>
<section id="further-resources">
<h2>Further Resources<a class="headerlink" href="#further-resources" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/tile-ai/tilelang">TileLang GitHub</a></p></li>
<li><p><a class="reference external" href="https://github.com/tile-ai/bitblas">BitBLAS</a></p></li>
<li><p><a class="reference external" href="https://github.com/openai/triton">Triton</a></p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/cutlass">Cutlass</a></p></li>
<li><p><a class="reference external" href="https://documen.tician.de/pycuda/">PyCUDA</a></p></li>
</ul>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="matmul_dequant.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">General Matrix-Matrix Multiplication with Dequantization</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="gemv.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">General Matrix-Vector Multiplication (GEMV)</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2025-2025, Tile Lang Contributors
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">General Matrix-Matrix Multiplication with Tile Library</a><ul>
<li><a class="reference internal" href="#why-another-gpu-dsl">Why Another GPU DSL?</a></li>
<li><a class="reference internal" href="#matrix-multiplication-example">Matrix Multiplication Example</a><ul>
<li><a class="reference internal" href="#basic-structure">Basic Structure</a></li>
<li><a class="reference internal" href="#key-concepts">Key Concepts</a></li>
</ul>
</li>
<li><a class="reference internal" href="#comparison-with-other-dsls">Comparison with Other DSLs</a></li>
<li><a class="reference internal" href="#performance-on-different-platforms">Performance on Different Platforms</a></li>
<li><a class="reference internal" href="#conclusion">Conclusion</a></li>
<li><a class="reference internal" href="#further-resources">Further Resources</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/scripts/furo.js"></script>
    </body>
</html>