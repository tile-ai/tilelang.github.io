<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html"><link rel="search" title="Search" href="../search.html"><link rel="next" title="Privacy" href="../privacy.html"><link rel="prev" title="General Matrix-Matrix Multiplication with Tile Library" href="matmul.html">
        <link rel="prefetch" href="../_static/img/logo-row.svg" as="image">
        <link rel="prefetch" href="../_static/img/logo-row.svg" as="image">

    <!-- Generated with Sphinx 8.1.3 and Furo 2025.09.25 -->
        <title>🚀 Write High Performance FlashMLA with TileLang on Hopper - Tile Language 0.1.6.post1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=580074bf" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=a5c4661c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=8dab3a3b" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle site navigation sidebar">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc" aria-label="Toggle table of contents sidebar">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <span class="icon"><svg><use href="#svg-menu"></use></svg></span>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">Tile Language <br> 0.1.6.post1 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  <div class="sidebar-logo-container">
    <img class="sidebar-logo only-light" src="../_static/img/logo-row.svg" alt="Light Logo"/>
    <img class="sidebar-logo only-dark" src="../_static/img/logo-row.svg" alt="Dark Logo"/>
  </div>
  
  <span class="sidebar-brand-text">Tile Language <br> 0.1.6.post1 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">GET STARTED</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/Installation.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/overview.html">The Tile Language: A Brief Introduction</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">TUTORIALS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/debug_tools_for_tilelang.html">Debugging Tile Language Programs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/auto_tuning.html">Auto-Tuning Techniques for Performance Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DEEP LEARNING OPERATORS</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="elementwise.html">ElementWise Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="gemv.html">General Matrix-Vector Multiplication (GEMV)</a></li>
<li class="toctree-l1"><a class="reference internal" href="matmul.html">General Matrix-Matrix Multiplication with Tile Library</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">🚀 Write High Performance FlashMLA with TileLang on Hopper</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../privacy.html">Privacy</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/deeplearning_operators/deepseek_mla.md.txt" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="write-high-performance-flashmla-with-tilelang-on-hopper">
<h1>🚀 Write High Performance FlashMLA with TileLang on Hopper<a class="headerlink" href="#write-high-performance-flashmla-with-tilelang-on-hopper" title="Link to this heading">¶</a></h1>
<div style="text-align: left;">
    <em>Author:</em> <a href="https://github.com/chengyupku">Yu Cheng</a> 
    <em>Author:</em> <a href="https://github.com/LeiWang1999">Lei Wang</a>
</div>
<p>TileLang is a user-friendly AI programming language that significantly lowers the barrier to kernel programming, helping users quickly build customized operators. However, users still need to master certain programming techniques to better leverage TileLang’s powerful capabilities. Here, we’ll use MLA as an example to demonstrate how to write high-performance kernels with TileLang.</p>
<section id="introduction-to-mla">
<h2>Introduction to MLA<a class="headerlink" href="#introduction-to-mla" title="Link to this heading">¶</a></h2>
<p>DeepSeek’s MLA (Multi-Head Latent Attention) is a novel attention mechanism known for its hardware efficiency and significant improvements in model inference speed. Several deep learning compilers (such as <a class="reference external" href="https://github.com/triton-lang/triton">Triton</a>) and libraries (such as <a class="reference external" href="https://github.com/flashinfer-ai/flashinfer">FlashInfer</a>) have developed their own implementations of MLA. In February 2025, <a class="reference external" href="https://github.com/deepseek-ai/FlashMLA">FlashMLA</a> was open-sourced on GitHub. FlashMLA utilizes <a class="reference external" href="https://github.com/NVIDIA/cutlass">CUTLASS</a> templates and incorporates optimization techniques from <a class="reference external" href="https://github.com/Dao-AILab/flash-attention">FlashAttention</a>, achieving impressive performance.</p>
</section>
<section id="benchmark-results">
<h2>Benchmark Results<a class="headerlink" href="#benchmark-results" title="Link to this heading">¶</a></h2>
<p>We benchmarked the performance of FlashMLA, TileLang, Torch, Triton, and FlashInfer under batch sizes of 64 and 128, with float16 data type, as shown in the figures below.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="../_images/bs64_float16.png"><img alt="Overview" src="../_images/bs64_float16.png" style="width: 50%;" />
</a>
<figcaption>
<p><span class="caption-text">Figure 1: Performance under batch size=64</span><a class="headerlink" href="#id1" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="../_images/bs128_float16.png"><img alt="Overview" src="../_images/bs128_float16.png" style="width: 50%;" />
</a>
<figcaption>
<p><span class="caption-text">Figure 2: Performance under batch size=128</span><a class="headerlink" href="#id2" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>As shown in the results, TileLang achieves performance comparable to FlashMLA in most cases, significantly outperforming both FlashInfer and Triton.
Notably, <strong>TileLang accomplishes this with just around 80 lines of Python code</strong>, demonstrating its exceptional ease of use and efficiency. Let’s dive in and see how TileLang achieves this.</p>
</section>
<section id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Link to this heading">¶</a></h2>
<p>First, let’s review the core computation logic of traditional FlashAttention:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># acc_s: [block_M, block_N]</span>
<span class="c1"># scores_max: [block_M]</span>
<span class="c1"># scores_scale: [block_M]</span>
<span class="c1"># acc_o: [block_M, dim]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">loop_range</span><span class="p">):</span>
    <span class="n">acc_s</span> <span class="o">=</span> <span class="n">Q</span> <span class="o">@</span> <span class="n">K</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">scores_max_prev</span> <span class="o">=</span> <span class="n">scores_max</span>
    <span class="n">scores_max</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">acc_s</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">scores_scale</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">scores_max_prev</span> <span class="o">-</span> <span class="n">scores_max</span><span class="p">)</span>
    <span class="n">acc_o</span> <span class="o">*=</span> <span class="n">scores_scale</span>
    <span class="n">acc_s</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">acc_s</span> <span class="o">-</span> <span class="n">scores_max</span><span class="p">)</span>
    <span class="n">acc_o</span> <span class="o">=</span> <span class="n">acc_s</span> <span class="o">@</span> <span class="n">V</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>Here, <code class="docutils literal notranslate"><span class="pre">acc_s</span></code> represents the <code class="docutils literal notranslate"><span class="pre">Q</span> <span class="pre">&#64;</span> <span class="pre">K</span></code> result in each iteration with dimensions <code class="docutils literal notranslate"><span class="pre">[block_M,</span> <span class="pre">block_N]</span></code>, while <code class="docutils literal notranslate"><span class="pre">acc_o</span></code> represents the current iteration’s output with dimensions <code class="docutils literal notranslate"><span class="pre">[block_M,</span> <span class="pre">dim]</span></code>. Both <code class="docutils literal notranslate"><span class="pre">acc_s</span></code> and <code class="docutils literal notranslate"><span class="pre">acc_o</span></code> need to be stored in registers to reduce latency.</p>
<p>Compared to traditional attention operators like MHA (Multi-Headed Attention) or GQA (Grouped Query Attention), a major challenge in optimizing MLA is its large head dimensions - <code class="docutils literal notranslate"><span class="pre">query</span></code> and <code class="docutils literal notranslate"><span class="pre">key</span></code> have head dimensions of 576 (512 + 64), while <code class="docutils literal notranslate"><span class="pre">value</span></code> has a head dimension of 512. This raises a significant issue: <code class="docutils literal notranslate"><span class="pre">acc_o</span></code> becomes too large, and with insufficient threads (e.g., 128 threads), register spilling occurs, severely impacting performance.</p>
<p>This raises the question of how to partition the matrix multiplication operation. On the Hopper architecture, most computation kernels use <a class="reference external" href="https://docs.nvidia.com/cuda/parallel-thread-execution/#asynchronous-warpgroup-level-matrix-instructions"><code class="docutils literal notranslate"><span class="pre">wgmma.mma_async</span></code></a> instructions for optimal performance. The <code class="docutils literal notranslate"><span class="pre">wgmma.mma_async</span></code> instruction organizes 4 warps (128 threads) into a warpgroup for collective MMA operations. However, <code class="docutils literal notranslate"><span class="pre">wgmma.mma_async</span></code> instructions require a minimum M dimension of 64. This means each warpgroup’s minimum M dimension can only be reduced to 64, but a tile size of 64*512 is too large for a single warpgroup, leading to register spilling.</p>
<p>Therefore, our only option is to partition <code class="docutils literal notranslate"><span class="pre">acc_o</span></code> along the <code class="docutils literal notranslate"><span class="pre">dim</span></code> dimension, with two warpgroups computing the left and right part of <code class="docutils literal notranslate"><span class="pre">acc_o</span></code> respectively. However, this introduces another challenge: both warpgroups require the complete <code class="docutils literal notranslate"><span class="pre">acc_s</span></code> result as input.</p>
<p>Our solution is to have each warpgroup compute half of <code class="docutils literal notranslate"><span class="pre">acc_s</span></code> during <code class="docutils literal notranslate"><span class="pre">Q</span> <span class="pre">&#64;</span> <span class="pre">K</span></code> computation, then obtain the other half computed by the other warpgroup through shared memory.</p>
<section id="layout-inference">
<h3>Layout Inference<a class="headerlink" href="#layout-inference" title="Link to this heading">¶</a></h3>
<p>While the above process may seem complex, but don’t worry - TileLang will handle all these intricacies for you.</p>
<p>Figure 3 and Figure 4 illustrate the frontend TileLang script and its corresponding execution plan for MLA. Here, <code class="docutils literal notranslate"><span class="pre">T.gemm</span></code> represents matrix multiplication operations, <code class="docutils literal notranslate"><span class="pre">transpose_B=True</span></code> indicates transposition of matrix B, and <code class="docutils literal notranslate"><span class="pre">policy=FullCol</span></code> specifies that each warpgroup computes one column (e.g., split the result matrix in vertical dimension). <code class="docutils literal notranslate"><span class="pre">T.copy</span></code> represents buffer-to-buffer copying operations.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="../_images/qk_layout.jpg"><img alt="Overview" src="../_images/qk_layout.jpg" style="width: 50%;" />
</a>
<figcaption>
<p><span class="caption-text">Figure 3: Buffer shapes in Q &#64; K</span><a class="headerlink" href="#id3" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="../_images/pv_layout.jpg"><img alt="Overview" src="../_images/pv_layout.jpg" style="width: 50%;" />
</a>
<figcaption>
<p><span class="caption-text">Figure 4: Buffer shapes in acc_s &#64; V</span><a class="headerlink" href="#id4" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>The mapping from TileLang frontend code to execution plan is accomplished through Layout Inference. Layout inference is a core optimization technique in TileLang. It automatically deduces the required buffer shapes and optimal layouts based on Tile-Operators (like <code class="docutils literal notranslate"><span class="pre">T.gemm</span></code>, <code class="docutils literal notranslate"><span class="pre">T.copy</span></code>, etc.), then generates the corresponding code. Here, we demonstrate a concrete example of buffer shape inference in MLA.</p>
<p>For instance, when computing <code class="docutils literal notranslate"><span class="pre">Q</span> <span class="pre">&#64;</span> <span class="pre">K</span></code>, TileLang infers that each warpgroup’s <code class="docutils literal notranslate"><span class="pre">acc_s_0</span></code> shape should be <code class="docutils literal notranslate"><span class="pre">[blockM,</span> <span class="pre">blockN</span> <span class="pre">/</span> <span class="pre">2]</span></code> based on the <code class="docutils literal notranslate"><span class="pre">policy=FullCol</span></code> annotation in <code class="docutils literal notranslate"><span class="pre">T.gemm</span></code>. Since this is followed by an <code class="docutils literal notranslate"><span class="pre">acc_s</span> <span class="pre">&#64;</span> <span class="pre">V</span></code> operation with <code class="docutils literal notranslate"><span class="pre">policy=FullCol</span></code>, which requires each warpgroup to have the complete <code class="docutils literal notranslate"><span class="pre">acc_s</span></code> result, TileLang deduces that <code class="docutils literal notranslate"><span class="pre">acc_s</span></code>’s shape at this point should be <code class="docutils literal notranslate"><span class="pre">[blockM,</span> <span class="pre">blockN]</span></code>. Consequently, TileLang can continue the inference process forward, determining that both <code class="docutils literal notranslate"><span class="pre">S_shared</span></code> and <code class="docutils literal notranslate"><span class="pre">acc_s</span></code> in <code class="docutils literal notranslate"><span class="pre">T.copy(S_shared,</span> <span class="pre">acc_s)</span></code> should have shapes of <code class="docutils literal notranslate"><span class="pre">[blockM,</span> <span class="pre">blockN]</span></code>.</p>
<p>It’s worth noting that our scheduling approach differs from FlashMLA’s implementation strategy. In FlashMLA, <code class="docutils literal notranslate"><span class="pre">Q</span> <span class="pre">&#64;</span> <span class="pre">K</span></code> is assigned to a single warpgroup, while the <code class="docutils literal notranslate"><span class="pre">acc_o</span></code> partitioning scheme remains consistent with ours. Nevertheless, our scheduling approach still achieves comparable performance.</p>
</section>
<section id="threadblock-swizzling">
<h3>Threadblock Swizzling<a class="headerlink" href="#threadblock-swizzling" title="Link to this heading">¶</a></h3>
<p>Threadblock swizzling is a common performance optimization technique in GPU kernel optimization. In GPU architecture, the L2 cache is a high-speed cache shared among multiple SMs (Streaming Multiprocessors). Threadblock swizzling optimizes data access patterns by remapping the scheduling order of threadblocks, thereby improving L2 cache hit rates. Traditional scheduling typically executes threadblocks in the natural order of the grid, which can lead to non-contiguous data access patterns between adjacent threadblocks, resulting in inefficient utilization of cached data. The swizzle technique employs mathematical mapping methods (such as diagonal or interleaved mapping) to adjust the execution order of threadblocks, ensuring that consecutively scheduled threadblocks access adjacent or overlapping data regions.</p>
<p>In TileLang, threadblock swizzling optimization can be implemented with just a single line of Python code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">T</span><span class="o">.</span><span class="n">use_swizzle</span><span class="p">(</span><span class="n">panel_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">order</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;row&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Here, <code class="docutils literal notranslate"><span class="pre">panel_size</span></code> specifies the width of the swizzled threadblock group, and <code class="docutils literal notranslate"><span class="pre">order</span></code> determines the swizzling pattern, which can be either “row” or “col”.</p>
</section>
<section id="shared-memory-swizzling">
<h3>Shared Memory Swizzling<a class="headerlink" href="#shared-memory-swizzling" title="Link to this heading">¶</a></h3>
<p>In CUDA programming, shared memory is divided into multiple memory banks, with each bank capable of servicing one thread request per clock cycle in parallel. Bank conflicts occur when multiple threads simultaneously access different addresses mapped to the same bank, forcing these accesses to be serialized and degrading performance.</p>
<p>One common strategy to address bank conflicts is shared memory swizzling. This technique rearranges how data is stored in shared memory by remapping addresses that would originally fall into the same bank to different banks, thereby reducing conflicts. For example, XOR operations or other bit manipulations can be incorporated into address calculations to alter the data layout, resulting in more evenly distributed memory accesses across consecutive threads. This approach is particularly crucial for implementing high-performance computing tasks like matrix multiplication and convolution, as it can significantly improve memory access parallelism and overall execution efficiency.</p>
<p>Similarly, TileLang also supports shared memory swizzling. Users only need to add a single line of Python code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">T</span><span class="o">.</span><span class="n">annotate_layout</span><span class="p">({</span>
    <span class="n">S_shared</span><span class="p">:</span> <span class="n">TileLang</span><span class="o">.</span><span class="n">layout</span><span class="o">.</span><span class="n">make_swizzled_layout</span><span class="p">(</span><span class="n">S_shared</span><span class="p">),</span>
<span class="p">})</span>
</pre></div>
</div>
<p>Here, <code class="docutils literal notranslate"><span class="pre">T.annotate_layout</span></code> allows users to specify any desired layout for a buffer. For convenience, TileLang provides the <code class="docutils literal notranslate"><span class="pre">make_swizzled_layout</span></code> primitive to automatically generate a swizzled layout.</p>
</section>
<section id="warp-specialization">
<h3>Warp-Specialization<a class="headerlink" href="#warp-specialization" title="Link to this heading">¶</a></h3>
<p>The Hopper architecture commonly employs warp specialization for performance optimization. A typical approach is to designate one warpgroup as a producer that handles data movement using TMA (Tensor Memory Accelerator), while the remaining warpgroups serve as consumers performing computations. However, this programming pattern is complex, requiring developers to manually manage the execution logic for producers and consumers, including synchronization through the <code class="docutils literal notranslate"><span class="pre">mbarrier</span></code> objects.</p>
<p>In TileLang, users are completely shielded from these implementation details. The frontend script is automatically transformed into a warp-specialized form, where TileLang handles all producer-consumer synchronization automatically, enabling efficient computation.</p>
</section>
<section id="pipeline">
<h3>Pipeline<a class="headerlink" href="#pipeline" title="Link to this heading">¶</a></h3>
<p>Pipeline is a technique used to improve memory access efficiency by overlapping memory access and computation. In TileLang, pipeline can be implemented through the <code class="docutils literal notranslate"><span class="pre">T.pipelined</span></code> annotation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">T</span><span class="o">.</span><span class="n">pipelined</span><span class="p">(</span><span class="nb">range</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">stage</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span>
</pre></div>
</div>
<p>Here, <code class="docutils literal notranslate"><span class="pre">range</span></code> specifies the range of the pipeline, and <code class="docutils literal notranslate"><span class="pre">stage</span></code> specifies the stage of the pipeline. Multi-stage pipelining enables overlapping of computation and memory access, which can significantly improve performance for memory-intensive operators. However, setting a higher number of stages consumes more shared memory resources, so the optimal configuration needs to be determined based on specific use cases.</p>
</section>
<section id="split-kv">
<h3>Split-KV<a class="headerlink" href="#split-kv" title="Link to this heading">¶</a></h3>
<p>We have also implemented Split-KV optimization similar to <a class="reference external" href="https://pytorch.org/blog/flash-decoding/">FlashDecoding</a>. Specifically, when the batch size is small, parallel SM resources cannot be fully utilized due to low parallelism. In such cases, we can split the kv_ctx dimension across multiple SMs for parallel computation and then merge the results.</p>
<p>In our implementation, we have developed both split and combine kernels, allowing users to control the split size through a <code class="docutils literal notranslate"><span class="pre">num_split</span></code> parameter.</p>
</section>
</section>
<section id="on-amd-mi300x-accelerators">
<h2>🚀 On AMD MI300X Accelerators<a class="headerlink" href="#on-amd-mi300x-accelerators" title="Link to this heading">¶</a></h2>
<p>Following our previous demonstration of <a class="reference external" href="https://github.com/tile-ai/tilelang/blob/main/examples/deepseek_mla/README.md">high-performance FlashMLA implementation on NVIDIA Hopper architectures using TileLang</a>, this work presents an optimized implementation for AMD MI300X accelerators. We examine architectural differences and corresponding optimization strategies between these platforms.</p>
<section id="architectural-considerations-and-optimization-strategies">
<h3>Architectural Considerations and Optimization Strategies<a class="headerlink" href="#architectural-considerations-and-optimization-strategies" title="Link to this heading">¶</a></h3>
<p>Key implementation differences between Hopper and MI300X architectures include:</p>
<ol class="arabic">
<li><p><strong>Instruction Set Variations</strong>: The MI300X architecture eliminates the need for explicit Tensor Memory Access (TMA) instructions and warp specialization, which are automatically handled by the compiler on Hopper architectures, resulting in identical source code manifestations.</p></li>
<li><p><strong>Shared Memory Constraints</strong>: With 64KB of shared memory compared to Hopper’s 228KB, MI300X implementations require careful memory management. Our optimization strategy includes:</p>
<ul class="simple">
<li><p>Reducing software pipeline stages</p></li>
<li><p>Register-based caching of Q matrices instead of shared memory utilization:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Original shared memory allocation</span>
<span class="n">Q_shared</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_shared</span><span class="p">([</span><span class="n">block_H</span><span class="p">,</span> <span class="n">dim</span><span class="p">],</span> <span class="n">dtype</span><span class="p">)</span>
<span class="n">Q_pe_shared</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_shared</span><span class="p">([</span><span class="n">block_H</span><span class="p">,</span> <span class="n">pe_dim</span><span class="p">],</span> <span class="n">dtype</span><span class="p">)</span>

<span class="c1"># Optimized register allocation</span>
<span class="n">Q_local</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_fragment</span><span class="p">([</span><span class="n">block_H</span><span class="p">,</span> <span class="n">dim</span><span class="p">],</span> <span class="n">dtype</span><span class="p">)</span>
<span class="n">Q_pe_local</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_fragment</span><span class="p">([</span><span class="n">block_H</span><span class="p">,</span> <span class="n">pe_dim</span><span class="p">],</span> <span class="n">dtype</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><strong>Tile Size Flexibility</strong>: The absence of WGMMA instructions on MI300X permits more flexible tile size selection, removing the requirement for block_m to be multiples of 64.</p></li>
<li><p><strong>Memory Bank Conflict Swizzling</strong>: MI300x has different memory bank conflict rules compared to NVIDIA, so we need to use different swizzling strategies. This is also automatically handled by TileLang, resulting in no visible differences in the code.</p></li>
</ol>
</section>
<section id="performance-evaluation">
<h3>Performance Evaluation<a class="headerlink" href="#performance-evaluation" title="Link to this heading">¶</a></h3>
<p>We conducted comparative performance analysis across multiple frameworks using float16 precision with batch sizes 64 and 128. The experimental results demonstrate:</p>
<figure style="text-align: center">
  <a href="../figures/flashmla-amd.png">
    <img src="../figures/flashmla-amd.png" alt="AMD FlashMLA Performance Comparison">
   </a>
  <figcaption style="text-align: center;">Figure 1: Computational throughput comparison across frameworks (Batch sizes 64 and 128)</figcaption>
</figure>
<p>Notably, TileLang achieves performance parity with hand-optimized assembly kernels (aiter-asm) in most test cases, while significantly outperforming both Triton (1.98×) and PyTorch (3.76×) implementations. This performance is achieved through a concise 80-line Python implementation, demonstrating TileLang’s efficiency and programmability advantages.</p>
</section>
<section id="future-optimization-opportunities">
<h3>Future Optimization Opportunities<a class="headerlink" href="#future-optimization-opportunities" title="Link to this heading">¶</a></h3>
<ol class="arabic simple">
<li><p><strong>Memory Bank Conflict Mitigation</strong>: Current implementations primarily address bank conflicts in NT layouts through TileLang’s automatic optimization. Further investigation of swizzling techniques for alternative memory layouts remains an open research direction.</p></li>
<li><p><strong>Dimension Parallelization</strong>: For large MLA dimensions (e.g., 576 elements), we propose investigating head dimension partitioning strategies to:</p>
<ul class="simple">
<li><p>Reduce shared memory pressure</p></li>
<li><p>Improve compute-to-memory access ratios</p></li>
<li><p>Enhance parallelism through dimension-wise task distribution</p></li>
</ul>
</li>
</ol>
</section>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="../privacy.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Privacy</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="matmul.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">General Matrix-Matrix Multiplication with Tile Library</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2025-2025, Tile Lang Contributors
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">🚀 Write High Performance FlashMLA with TileLang on Hopper</a><ul>
<li><a class="reference internal" href="#introduction-to-mla">Introduction to MLA</a></li>
<li><a class="reference internal" href="#benchmark-results">Benchmark Results</a></li>
<li><a class="reference internal" href="#implementation">Implementation</a><ul>
<li><a class="reference internal" href="#layout-inference">Layout Inference</a></li>
<li><a class="reference internal" href="#threadblock-swizzling">Threadblock Swizzling</a></li>
<li><a class="reference internal" href="#shared-memory-swizzling">Shared Memory Swizzling</a></li>
<li><a class="reference internal" href="#warp-specialization">Warp-Specialization</a></li>
<li><a class="reference internal" href="#pipeline">Pipeline</a></li>
<li><a class="reference internal" href="#split-kv">Split-KV</a></li>
</ul>
</li>
<li><a class="reference internal" href="#on-amd-mi300x-accelerators">🚀 On AMD MI300X Accelerators</a><ul>
<li><a class="reference internal" href="#architectural-considerations-and-optimization-strategies">Architectural Considerations and Optimization Strategies</a></li>
<li><a class="reference internal" href="#performance-evaluation">Performance Evaluation</a></li>
<li><a class="reference internal" href="#future-optimization-opportunities">Future Optimization Opportunities</a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=c58fda0c"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=46bd48cc"></script>
    </body>
</html>