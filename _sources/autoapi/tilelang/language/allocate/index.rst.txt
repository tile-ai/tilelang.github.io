tilelang.language.allocate
==========================

.. py:module:: tilelang.language.allocate

.. autoapi-nested-parse::

   Memory allocation utilities for Tile-AI programs.

   This module provides a set of functions for allocating different types of memory buffers
   in Tile-AI programs. It wraps TVM's buffer allocation functionality with convenient
   interfaces for different memory scopes.

   Available allocation functions:
       - alloc_shared: Allocates shared memory buffers for inter-thread communication
       - alloc_local: Allocates local memory buffers for thread-private storage
       - alloc_fragment: Allocates fragment memory buffers for specialized operations
       - alloc_var: Allocates single-element variable buffers

   Each function takes shape and dtype parameters and returns a TVM buffer object
   with the appropriate memory scope.



Functions
---------

.. autoapisummary::

   tilelang.language.allocate.alloc_shared
   tilelang.language.allocate.alloc_local
   tilelang.language.allocate.alloc_fragment
   tilelang.language.allocate.alloc_var
   tilelang.language.allocate.alloc_barrier
   tilelang.language.allocate.alloc_tmem
   tilelang.language.allocate.alloc_reducer


Module Contents
---------------

.. py:function:: alloc_shared(shape, dtype, scope='shared.dyn')

   Allocate a shared memory buffer for inter-thread communication.

   :param shape: The shape of the buffer to allocate
   :type shape: tuple
   :param dtype: The data type of the buffer (e.g., 'float32', 'int32')
   :type dtype: str
   :param scope: The memory scope. Defaults to "shared.dyn"
   :type scope: str, optional

   :returns: A TVM buffer object allocated in shared memory
   :rtype: T.Buffer


.. py:function:: alloc_local(shape, dtype, scope='local')

   Allocate a local memory buffer for thread-private storage.

   :param shape: The shape of the buffer to allocate
   :type shape: tuple
   :param dtype: The data type of the buffer (e.g., 'float32', 'int32')
   :type dtype: str
   :param scope: The memory scope. Defaults to "local"
   :type scope: str, optional

   :returns: A TVM buffer object allocated in local memory
   :rtype: T.Buffer


.. py:function:: alloc_fragment(shape, dtype, scope='local.fragment')

   Allocate a fragment memory buffer for specialized operations.

   :param shape: The shape of the buffer to allocate
   :type shape: tuple
   :param dtype: The data type of the buffer (e.g., 'float32', 'int32')
   :type dtype: str
   :param scope: The memory scope. Defaults to "local.fragment"
   :type scope: str, optional

   :returns: A TVM buffer object allocated in fragment memory
   :rtype: T.Buffer


.. py:function:: alloc_var(dtype, scope='local.var')

   Allocate a single-element variable buffer.

   :param dtype: The data type of the buffer (e.g., 'float32', 'int32')
   :type dtype: str
   :param scope: The memory scope. Defaults to "local.var"
   :type scope: str, optional

   :returns: A TVM buffer object allocated as a single-element variable
   :rtype: T.Buffer


.. py:function:: alloc_barrier(arrive_count)

   Allocate a barrier buffer.

   :param arrive_count: The number of threads that need to arrive at the barrier
   :type arrive_count: int

   :returns: A TVM buffer object allocated as a barrier
   :rtype: T.Buffer


.. py:function:: alloc_tmem(shape, dtype)

   Allocate a Tensor Memory (TMEM) buffer for use with 5th generation Tensor Core operations (e.g., UMMA).

   TMEM is a dedicated on-chip memory introduced in Hopper GPUs, designed to reduce register pressure and enable asynchronous, single-threaded MMA operations. It is organized as a 2D array of 512 columns by 128 rows (lanes), with each cell being 32 bits. Allocation is performed in units of columns, and every lane of a column is allocated together.

   Key properties and requirements:
       - The number of columns allocated must be a power of 2 and at least 32.
       - TMEM allocations are dynamic and must be explicitly deallocated.
       - Both allocation and deallocation must be performed by the same warp.
       - The base address of the TMEM allocation is stored in shared memory and used as the offset for UMMA accumulator tensors.
       - Only UMMA and specific TMEM load/store instructions can access TMEM; all pre-processing must occur before data is loaded into TMEM, and all post-processing after data is retrieved.
       - The number of columns allocated should not increase between any two allocations in the execution order within the CTA.

   :param num_cols: Number of columns to allocate in TMEM. Must be a power of 2 and >= 32 but less than or equal to 512.
   :type num_cols: int

   :returns: A TVM buffer object allocated in TMEM scope, suitable for use as an accumulator or operand in UMMA operations.
   :rtype: T.Buffer

   .. note::

      - TMEM is only available on supported architectures (e.g., Hopper and later).
      - The buffer returned should be used according to TMEM access restrictions and deallocated appropriately.


.. py:function:: alloc_reducer(shape, dtype, op='sum', replication=None)

   Allocate a reducer buffer.

   Modifications needs to conform with `op`,
   such as `op="sum"` requires `reducer[...] += ...` and
   `op="max"` requires `reducer[...] = T.max(reducer[...], ...)`.

   Only after T.fill with proper initializer the reduction may begin;
   only after T.finalize_reducer the partial results will be available.

   For `op="sum"`, filled value must be 0; for min and max, the filled initializer will become max or min clamper correspondingly.
   You may want to use `T.max_value` for min and `T.min_value` for max.

   :param shape: The shape of the buffer to allocate
   :type shape: tuple
   :param dtype: The data type of the buffer (e.g., 'float32', 'int32')
   :type dtype: str
   :param op: The reduce operation corresponded with the reducer
   :type op: str
   :param replication: Replication strategy, can be "all" or "none". Defaults to not specified, and the compiler will do whatever it want.
   :type replication: str | None

   :returns: A TVM buffer object allocated in thread-private storage, available to reduce values in T.Parallel loops.
   :rtype: T.Buffer


